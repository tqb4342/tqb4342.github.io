<!DOCTYPE html>
<html>
<meta  lang="zh-CN" >
<head>
  <meta charset="UTF-8">
  <meta name="viewport"
    content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#fff" id="theme-color">
  <link rel="icon" href="/img/logo.jpg">
  <title>【无监督学习之线性降维】Unsupervised Learning-Linear Dimension Reduction&nbsp; | &nbsp;IT码农</title>
  
  
  <meta property="title" content="【无监督学习之线性降维】Unsupervised Learning-Linear Dimension Reduction">
  
  
  <meta property="url" content="https://tanqingbo.cn/Unsupervised-Learning-Linear-Dimension-Reduction/index.html">
  
  
  <meta property="og:img" content="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203448.png">
  
  
  <meta property="description" content="本节主要介绍了**两种线性降维的方法**：**Cluster和PCA**,并从两个角度解释了PCA。">
  
  <meta property="keywords" content="机器学习,医学图像,深度学习">
  
  <meta property="og:type" content="article">
  <meta property="og:article:published_time" content="2020-12-29">
  <meta property="og:article:modified_time" content="2020-12-29">
  <meta property="og:article:author" content="IT&amp;nbsp; 码&amp;nbsp; 农">
  
  
  <meta property="og:article:tag" content="机器学习">
  
  <meta property="og:article:tag" content="深度学习">
  
  
  
  
  
 <meta name="yandex-verification" content="6b899e6f05914428" /> 
 <link rel="canonical" href="https://tanqingbo.cn/Unsupervised-Learning-Linear-Dimension-Reduction/index.html" />
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178024758-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178024758-1');
</script>

<!-- 百度自动推送 -->
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

<!-- 360自动推送 -->
<script>
(function(){
var src = "https://jspassport.ssl.qhimg.com/11.0.1.js?d182b3f28525f2db83acfaaf6e696dba";
document.write('<script src="' + src + '" id="sozz"><\/script>');
})();
</script>

  
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d87293651eefff721c898f38338ea3a8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
    var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
      }
    };
    setDarkmode();
  </script>
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
  </script>
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_f7g5jnuftcf.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  <link rel="preload" href="//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js" as="script">
  
  
  <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
  
  <link rel="prefetch" href="//cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-svg.js" as="script">
  
  
  
  <link rel="prefetch" href="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js" as="script">
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_f7g5jnuftcf.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">

  
  
  
<meta name="generator" content="Hexo 5.1.1">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>


<body>
  <div class="wrapper">
    
    <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
      <img class="navbar-logo-img" src="/img/logo.jpg">
      
      <span class="navbar-logo-dsc">IT&nbsp; 码&nbsp; 农</span>
    </span>
  </div>
  <div class="navbar-menu">
    
    <a href="/" class="navbar-menu-item">
    
    首页
    
    </a>
    
    <a href="/CSBook001" class="navbar-menu-item">
    
    电子书下载
    
    </a>
    
    <a href="/categories" class="navbar-menu-item">
    
    分类
    
    </a>
    
    <a href="/links" class="navbar-menu-item">
    
    友链
    
    </a>
    
    <a class="navbar-menu-item darknavbar" id="dark"><i class="iconfont icon-weather"></i></a>
    <a class="navbar-menu-item searchnavbar" id="search"><i class="iconfont icon-search" style="font-size: 1.2rem; font-weight: 400;"></i></a>
  </div>
</nav>
    
    <div id="local-search" style="display: none;">
      <input class="navbar-menu-item" id="search-input" placeholder="请输入搜索内容...">
      <div id="search-content"></div>
    </div>
    
    <div class="section-wrap">
      <div class="container">
        <div class="columns">
          <main class="main-column">
<div class="image-wrapper">
  <img src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203448.png" data-src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203448.png"
    srcset="data:image/svg+xml,%3Csvg%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20viewBox=&#39;0%200%20300%20300&#39;%3E%3C/svg%3E"
    class="image lozad">
</div>

<article class="card card-content">
  <header>
    <h1 class="post-title">
      【无监督学习之线性降维】Unsupervised Learning-Linear Dimension Reduction
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2020-12-29T12:33:18.000Z" style="display: flex; align-items: center;">
      <i class="iconfont icon-calendar" style="margin-right: 2px;"></i>
      <span>2020-12-29</span>
    </time>
    
    <span class="dot"></span>
    
    <a href="/categories/图像处理与机器学习/" class="post-meta-link">图像处理与机器学习</a>
    
    
    
    <span class="dot"></span>
    <span>1.1k 字</span>
    
	&nbsp;&nbsp;
	  <!-- id 将作为查询条件 -->
    <div style="display: flex; align-items: center;">
      <i class="iconfont icon-wodebaobiao" style="margin-right: 2px; font-size: 1.15rem;"></i>
		<span id="/Unsupervised-Learning-Linear-Dimension-Reduction/" class="leancloud_visitors" data-flag-title="【无监督学习之线性降维】Unsupervised Learning-Linear Dimension Reduction">
			<em class="post-meta-item-text">阅读量 </em>
			<i class="leancloud-visitors-count">loading</i>
		</span>
	</div>
	
  </div>
  
  
  
  <div class="post-meta post-show-meta" style="margin-top: -10px;">
    <div style="display: flex; align-items: center;">
      <i class="iconfont icon-biaoqian" style="margin-right: 2px; font-size: 1.15rem;"></i>
      
      
        <a href="/tags/机器学习/" class="post-meta-link">机器学习</a>
      
      
      <span class="dot"></span>
      
        <a href="/tags/深度学习/" class="post-meta-link">深度学习</a>
      
    </div>
  </div>
  
  


		
  
  <meta name="description" content="本节主要介绍了**两种线性降维的方法**：**Cluster和PCA**,并从两个角度解释了PCA。">
  
  <meta name="keywords" content="机器学习,医学图像,深度学习">
  </header>
  

  
  <div id="section" class="post-content">
	<!--
		<p>最近整理了一些计算专业必读的经典专业书，可以点击下面的链接查看和下载高清pdf电子版：</p>
		<ul>
		<li><a href="https://tanqingbo.cn/computer-book/">计算机专业几本必看的几本经典书籍！</a></li>
		</ul>
	-->
    <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><ul>
<li>本节主要介绍了<strong>两种线性降维的方法</strong>：<strong>Cluster和PCA</strong>,并从两个角度解释了PCA。</li>
</ul>
<h2 id="聚类-Cluster"><a href="#聚类-Cluster" class="headerlink" title="聚类(Cluster)"></a>聚类(Cluster)</h2><ul>
<li>聚类的基本思想是将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个**”簇”(cluster)**。<h3 id="K均值算法（K-means）"><a href="#K均值算法（K-means）" class="headerlink" title="K均值算法（K-means）"></a>K均值算法（K-means）</h3></li>
</ul>
<ol>
<li>随机初始化K个样本(点)，称之为簇中心(cluster centroids)；</li>
<li>簇分配: 对于所有的样本，将其分配给离它最近的簇中心；</li>
<li>移动簇中心：对于每一个簇，计算属于该簇的所有样本的平均值，移动簇中心到平均值处；</li>
<li>重复步骤2和3，直到找到我们想要的簇.</li>
<li>如下图演示了特征量个数和簇数均为2的情况：</li>
</ol>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229202921.gif" class="lozad post-image"src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229202921.gif"></p>
<h3 id="分层凝聚聚类（Hierarchical-Agglomerative-Clustering-HAC）原理"><a href="#分层凝聚聚类（Hierarchical-Agglomerative-Clustering-HAC）原理" class="headerlink" title="分层凝聚聚类（Hierarchical Agglomerative Clustering,HAC）原理"></a>分层凝聚聚类（Hierarchical Agglomerative Clustering,HAC）原理</h3><ul>
<li>顾名思义就是要一层一层地进行聚类，可以从下而上地把小的cluster合并聚集，也可以从上而下地将大的cluster进行分割。似乎一般用得比较多的是从下而上地聚集，因此这里我就只介绍这一种。</li>
<li>所谓从下而上地合并cluster，具体而言，就是每次找到距离最短的两个cluster，然后进行合并成一个大的cluster，直到全部合并为一个cluster。整个过程就是建立一个树结构，类似于下图。 </li>
</ul>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229202937.jpeg" class="lozad post-image"src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229202937.jpeg"></p>
<ul>
<li>层次聚类最大的优点，就是它一次性地得到了整个聚类的过程，只要得到了上面那样的聚类树，想要分多少个cluster都可以直接根据树结构来得到结果，改变cluster数目不需要再次计算数据点的归属。层次聚类的缺点是计算量比较大，因为要每次都要计算多个cluster内所有数据点的两两距离。另外，由于层次聚类使用的是贪心算法，得到的显然只是局域最优，不一定就是全局最优，这可以通过加入随机效应解决，这就是另外的问题了。</li>
</ul>
<h2 id="主成分分析-Principle-Component-Analysis-PCA"><a href="#主成分分析-Principle-Component-Analysis-PCA" class="headerlink" title="主成分分析(Principle Component Analysis,PCA)"></a>主成分分析(Principle Component Analysis,PCA)</h2><ul>
<li>PCA降维原理可以从<strong>两个角度来考虑</strong>：<ol>
<li><strong>基于最大方差原理</strong>，样本点在这个超平面上的投影尽可能分开。</li>
<li><strong>基于最小化误差原理</strong>，样本点到这个超平面距离都足够近。</li>
</ol>
</li>
</ul>
<h3 id="基于最大方差原理"><a href="#基于最大方差原理" class="headerlink" title="基于最大方差原理"></a>基于最大方差原理</h3><ul>
<li>需要找到一个投影矩阵W，使得x在W上的投影方差尽可能大，其中W是由多个向量组成，其中W是由多个向量组成(w1,w2,w3…),希望x在w1上的投影的方差最大，w2上的投影的方差其次…..依次类推。</li>
</ul>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203131.jpeg" class="lozad post-image"src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203131.jpeg"></p>
<ul>
<li>并且，W是一个单位正交矩阵，即<strong>（w1,w2,w3,…）相互正交，且都是单位向量</strong>。</li>
</ul>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203142.png" class="lozad post-image"src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203142.png"></p>
<ul>
<li><strong>PCA达到的效果就是decorrelation（去关联）</strong>，所以最后投影之后得到z的协方差矩阵D是对角矩阵； <ul>
<li><strong>投影矩阵W是单位正交矩阵。</strong></li>
<li><strong>W就是x协方差矩阵S的特征向量。</strong></li>
</ul>
</li>
</ul>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203158.jpeg" class="lozad post-image"src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203158.jpeg"></p>
<h3 id="基于最小化误差原理"><a href="#基于最小化误差原理" class="headerlink" title="基于最小化误差原理"></a>基于最小化误差原理</h3><ul>
<li>基本思想：将<img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://pic1.zhimg.com/80/v2-d74c74db128d168f2475e0a04d588f7c_720w.jpg" class="lozad post-image"src="https://pic1.zhimg.com/80/v2-d74c74db128d168f2475e0a04d588f7c_720w.jpg">近似看成由多个u组成，求解最小化他们之间的error时的系数c和分量u。</li>
<li>其中向量(u1,u2,u3…)表示一个<code>Basic component</code>,如下图：</li>
</ul>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203223.jpeg" class="lozad post-image"src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203223.jpeg"><br><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203232.jpeg" class="lozad post-image"src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203232.jpeg"></p>
<ul>
<li>为了求解c和u（component），可以将X做奇异值分解SVD，用分解后的U代替u，ΣxV代替系数c<br>其中U就是XXT的特征向量 </li>
<li>有时候只选取特征值比较大的<code>component</code>。</li>
<li>PCA相当于只含一层hidden layer的网络。</li>
</ul>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203244.png" class="lozad post-image"src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201229203244.png"></p>
<h2 id="PCA与LDA-Linear-Discriminant-Analysis-的比较"><a href="#PCA与LDA-Linear-Discriminant-Analysis-的比较" class="headerlink" title="PCA与LDA(Linear Discriminant Analysis)的比较"></a>PCA与LDA(Linear Discriminant Analysis)的比较</h2><ul>
<li>PCA是无监督的，LDA是有监督的；</li>
<li>PCA基本思想是方差最大，LDA基本思想是让不同类别分的尽可能开；</li>
<li>PCA和LDA都是线性映射；</li>
<li>对于结构比较复杂的降维，只能采用非线性流行学习比如局部线性嵌入(Locally Linear Embedding，LLE)等方法.</li>
</ul>
<h2 id="PCA和非负矩阵分解-Non-negative-Matrix-Factorization-NMF-比较"><a href="#PCA和非负矩阵分解-Non-negative-Matrix-Factorization-NMF-比较" class="headerlink" title="PCA和非负矩阵分解(Non-negative Matrix Factorization,NMF)比较"></a>PCA和非负矩阵分解(Non-negative Matrix Factorization,NMF)比较</h2><ul>
<li>NMF分解之后的component的系数都是正的，就拿image来说，也就是说分解之后的component像是原始image的一部分;</li>
<li>而PCA的系数可正可负，涉及到component的“加加减减” .</li>
</ul>
		
  </div>	
<div>
  	  <br>
	<div style="height:20px;border:none;border-top:1px solid #000;"></div>  
	<!--
    <div style="margin-left: 125px;">
    <div style="float: left; width: 200px; height: 200px;">
        <img class="adv" style="width: 100%; height: 100%; border: solid 1px #ddd;" src="https://pic4.zhimg.com/80/v2-6f4153925b20af12b17837b531302695_720w.jpeg">
    </div>
	
    <div style="float: left; margin-left: 35px;">
        <div style="font-size: 28px; margin-bottom: 12px;margin-left: 4px;">关注我的公众号</div>
        <small style="font-size: 1.2em;">
            →「技术干货」每日推送<br><br>
            →「免费资料」随时领取<br><br>
            →「签到活动」每周福利<br><br>
        </small>
    </div>
    <div style="clear: both;"></div>	
	</div>	
	-->
  	<p align="center" style="margin-top: 15px; font-size: 16px;color: #337ab7;">
       <strong><a href="https://tanqingbo.cn/CSBook001/" target="_blank" style="unicode-bidi: isolate;font-variant-numeric: tabular-nums;text-transform: none;text-decoration-line: underline;">点击下载：计算机专业最全PDF高清电子书！</a></strong>
    </p>
  
<div>
  
  <div class="post-note note-warning copyright" style="margin-top: 42px">
    <p><span style="font-weight: bold;">作者：</span><a target="_blank" rel="nofollow noopener noreferrer" href="https://tanqingbo.cn/about">IT码农</a></p>
    <p><span style="font-weight: bold;">文章链接：</span><a target="_blank" rel="nofollow noopener noreferrer" href="https://tanqingbo.cn/Unsupervised-Learning-Linear-Dimension-Reduction/">https://tanqingbo.cn/Unsupervised-Learning-Linear-Dimension-Reduction/</a></p>
    <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener external nofollow noreferrer" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
  </div>
  
  </div>
</article>
<div class="nav">
  
  <div class="nav-item-prev">
    <a href="/Word-Embedding/" class="nav-link">
      <i class="iconfont icon-left nav-prev-icon"></i>
      <div>
        <div class="nav-label">上一篇</div>
        
        <div class="nav-title">无监督学习之词嵌入or词向量(Word Embedding) </div>
        
      </div>
    </a>
  </div>
  
  
  <div class="nav-item-next">
    <a href="/Semi-supervised-learning/" class="nav-link">
      <div>
        <div class="nav-label">下一篇</div>
        
        <div class="nav-title">机器学习之半监督学习（Semi-supervised learning） </div>
        
      </div>
      <i class="iconfont icon-right nav-next-icon"></i>
    </a>
  </div>
  
</div>

<div class="card card-content comment-card" style="margin-top: 16px;">
  <div class="comment-card-title">评论</div>
  
  <div id="vcomments"></div>
  
  <script>
    loadScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js");
    var oldLoadVa = window.onload;
    window.onload = function () {
      oldLoadVa && oldLoadVa();
      new Valine({
        el: '#vcomments',
        appId: 'RtYsYDbddUcoEbJrbQcpPgP2-gzGzoHsz',
        appKey: 'M8e7BRAbupxzxRVSoj9JkJ5E',
        placeholder: 'Just go go',
        path: window.location.pathname,
        avatar: 'mp',
        meta: ["nick","mail","link"],
        pageSize: '10',
        lang: '',
        visitor: 'true',
        highlight: true,
        recordIP: false,
        
        
        
        enableQQ: 'false',
        requiredFields: [],
      });
    };
  </script>

</div>

<div class="card card-content toc-card" id="mobiletoc">
  <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB-Cluster"><span class="toc-text">聚类(Cluster)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%EF%BC%88K-means%EF%BC%89"><span class="toc-text">K均值算法（K-means）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B1%82%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB%EF%BC%88Hierarchical-Agglomerative-Clustering-HAC%EF%BC%89%E5%8E%9F%E7%90%86"><span class="toc-text">分层凝聚聚类（Hierarchical Agglomerative Clustering,HAC）原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-Principle-Component-Analysis-PCA"><span class="toc-text">主成分分析(Principle Component Analysis,PCA)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%A4%A7%E6%96%B9%E5%B7%AE%E5%8E%9F%E7%90%86"><span class="toc-text">基于最大方差原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%B0%8F%E5%8C%96%E8%AF%AF%E5%B7%AE%E5%8E%9F%E7%90%86"><span class="toc-text">基于最小化误差原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PCA%E4%B8%8ELDA-Linear-Discriminant-Analysis-%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">PCA与LDA(Linear Discriminant Analysis)的比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PCA%E5%92%8C%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3-Non-negative-Matrix-Factorization-NMF-%E6%AF%94%E8%BE%83"><span class="toc-text">PCA和非负矩阵分解(Non-negative Matrix Factorization,NMF)比较</span></a></li></ol>
</div></main>
          <aside class="left-column">
            
            <div class="card card-author">
              
<img src="/img/logo.jpg" class="author-img">

<p class="author-name">IT&nbsp; 码&nbsp; 农</p>
<p class="author-description">一个专注于程序员成长的网站</p>
<div class="author-message">
  <a class="author-posts-count" href="/archives">
    <span>135</span>
    <span>文章</span>
  </a>
  <a class="author-categories-count" href="/categories">
    <span>13</span>
    <span>分类</span>
  </a>
  <a class="author-tags-count" href="/tags">
    <span>91</span>
    <span>标签</span>
  </a>
</div>

<div class="author-card-society">
  
    <div class="author-card-society-icon">
      <a target="_blank" rel="noopener external nofollow noreferrer" href="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20201215143241.png">
        <i class="iconfont icon-wechat society-icon"></i>
      </a>
    </div>
  
    <div class="author-card-society-icon">
      <a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/tqb4342">
        <i class="iconfont icon-github society-icon"></i>
      </a>
    </div>
  
    <div class="author-card-society-icon">
      <a target="_blank" rel="noopener external nofollow noreferrer" href="https://weibo.com/tqb4342">
        <i class="iconfont icon-sina society-icon"></i>
      </a>
    </div>
  
</div>

            </div>
            
            <div class="sticky-tablet">
  
  
  <article class="display-when-two-columns spacer">
    <div class="card card-content toc-card">
      <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB-Cluster"><span class="toc-text">聚类(Cluster)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%EF%BC%88K-means%EF%BC%89"><span class="toc-text">K均值算法（K-means）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B1%82%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB%EF%BC%88Hierarchical-Agglomerative-Clustering-HAC%EF%BC%89%E5%8E%9F%E7%90%86"><span class="toc-text">分层凝聚聚类（Hierarchical Agglomerative Clustering,HAC）原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-Principle-Component-Analysis-PCA"><span class="toc-text">主成分分析(Principle Component Analysis,PCA)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%A4%A7%E6%96%B9%E5%B7%AE%E5%8E%9F%E7%90%86"><span class="toc-text">基于最大方差原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%B0%8F%E5%8C%96%E8%AF%AF%E5%B7%AE%E5%8E%9F%E7%90%86"><span class="toc-text">基于最小化误差原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PCA%E4%B8%8ELDA-Linear-Discriminant-Analysis-%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">PCA与LDA(Linear Discriminant Analysis)的比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PCA%E5%92%8C%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3-Non-negative-Matrix-Factorization-NMF-%E6%AF%94%E8%BE%83"><span class="toc-text">PCA和非负矩阵分解(Non-negative Matrix Factorization,NMF)比较</span></a></li></ol>
    </div>
  </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header"><i class="iconfont icon-fenlei" style="padding-right: 2px;"></i>分类</div>
  <div class="categories-list">
    
      <a href="/categories/工具">
        <div class="categories-list-item">
          工具
          <span class="categories-list-item-badge">6</span>
        </div>
      </a>
    
      <a href="/categories/设计模式">
        <div class="categories-list-item">
          设计模式
          <span class="categories-list-item-badge">5</span>
        </div>
      </a>
    
      <a href="/categories/数据结构与算法">
        <div class="categories-list-item">
          数据结构与算法
          <span class="categories-list-item-badge">5</span>
        </div>
      </a>
    
      <a href="/categories/C与C✙✙">
        <div class="categories-list-item">
          C与C✙✙
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/编程资料">
        <div class="categories-list-item">
          编程资料
          <span class="categories-list-item-badge">25</span>
        </div>
      </a>
    
      <a href="/categories/Python">
        <div class="categories-list-item">
          Python
          <span class="categories-list-item-badge">7</span>
        </div>
      </a>
    
      <a href="/categories/ITK与VTK">
        <div class="categories-list-item">
          ITK与VTK
          <span class="categories-list-item-badge">7</span>
        </div>
      </a>
    
      <a href="/categories/Java">
        <div class="categories-list-item">
          Java
          <span class="categories-list-item-badge">19</span>
        </div>
      </a>
    
      <a href="/categories/Linux">
        <div class="categories-list-item">
          Linux
          <span class="categories-list-item-badge">5</span>
        </div>
      </a>
    
      <a href="/categories/SEO教程">
        <div class="categories-list-item">
          SEO教程
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/图像处理与机器学习">
        <div class="categories-list-item">
          图像处理与机器学习
          <span class="categories-list-item-badge">43</span>
        </div>
      </a>
    
      <a href="/categories/计算机基础知识">
        <div class="categories-list-item">
          计算机基础知识
          <span class="categories-list-item-badge">7</span>
        </div>
      </a>
    
      <a href="/categories/技术以外">
        <div class="categories-list-item">
          技术以外
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header"><i class="iconfont icon-biaoqian" style="padding-right: 2px;"></i>热门标签</div>
  <div class="tags-list">
    
    <a href="\tags\机器学习" title="机器学习"><div class="tags-list-item">机器学习</div></a>
    
    <a href="\tags\Java" title="Java"><div class="tags-list-item">Java</div></a>
    
    <a href="\tags\算法" title="算法"><div class="tags-list-item">算法</div></a>
    
    <a href="\tags\深度学习" title="深度学习"><div class="tags-list-item">深度学习</div></a>
    
    <a href="\tags\ITK" title="ITK"><div class="tags-list-item">ITK</div></a>
    
    <a href="\tags\Python" title="Python"><div class="tags-list-item">Python</div></a>
    
    <a href="\tags\电子书下载" title="电子书下载"><div class="tags-list-item">电子书下载</div></a>
    
    <a href="\tags\VTK" title="VTK"><div class="tags-list-item">VTK</div></a>
    
    <a href="\tags\设计模式" title="设计模式"><div class="tags-list-item">设计模式</div></a>
    
    <a href="\tags\Linux" title="Linux"><div class="tags-list-item">Linux</div></a>
    
    <a href="\tags\Keras" title="Keras"><div class="tags-list-item">Keras</div></a>
    
    <a href="\tags\图谱分割" title="图谱分割"><div class="tags-list-item">图谱分割</div></a>
    
    <a href="\tags\操作系统" title="操作系统"><div class="tags-list-item">操作系统</div></a>
    
    <a href="\tags\工具" title="工具"><div class="tags-list-item">工具</div></a>
    
    <a href="\tags\面试" title="面试"><div class="tags-list-item">面试</div></a>
    
    <a href="\tags\数据结构" title="数据结构"><div class="tags-list-item">数据结构</div></a>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
          <aside class="right-column">
            <div class="sticky-widescreen">


<article class="card card-content toc-card">
<div style="">
    <div style="float: left; width: 125px; height: 125px;">
        <img class="adv" style="width: 100%; height: 100%; border: solid 1px #ddd;" src="https://pic4.zhimg.com/80/v2-6f4153925b20af12b17837b531302695_720w.jpeg">
    </div>
    <div style="float: left; margin-left: 17px; margin-top: 12px;">
        <div style="font-size: 18px; margin-bottom: 12px;margin-left: 4px;">关注我的公众号</div>
        <small style="font-size: 0.8em;">
            →「技术干货」每日推送<br>
            →「免费资料」随时领取<br>
            →「签到活动」每周福利<br>
        </small>
    </div>
    <div style="clear: both;"></div>
	<div style="font-size: 0.8em; margin: 15px 0 10px 0;">
    <a href="" target="_blank" style="color:orange;font-size: 14px;" rel="noopener noreferrer">扫描上方二维码，关注我的公众号</a>
    <br>
    <div style="height: 8px;"></div>
    更多干货，等你来看，我在微信上等你！
</div>
</div>
</article>


<article class="card card-content toc-card">
<div class="xingqiu-img"> <a href="https://t.1yb.co/dVQD" rel="external nofollow noreferrer" target="_blank"><img src="https://cdn.jsdelivr.net/gh/tqb4342/BlogPhoto@master/20210104122757.png"></a></div>
<!--
<div style="clear: both;"></div>
<div style="font-size: 0.8em; margin: 15px 0 10px 0;">
	<strong><span style="caret-color: red;font-size: 15px;color: rgb(77, 168, 238);"><a href="https://tanqingbo.cn/about/" title="计算机专业经典书籍下载">扫码加入星球你将收获：</a></span></strong>
    <div style="height: 8px;"></div>
    1、主流互联网中找不到的副业赚钱方法，帮助你闷声赚钱；
	<div style="height: 5px;"></div>
	2、分享我的投资理财方法以及互联网中各种流量陷阱和套路；
	<div style="height: 5px;"></div>
	3、无限次向我提问，咨询成长方法、求学困惑、副业赚钱等问题。
</div>
-->
</article>


  
  
  <article class="card card-content toc-card">
    <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB-Cluster"><span class="toc-text">聚类(Cluster)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%EF%BC%88K-means%EF%BC%89"><span class="toc-text">K均值算法（K-means）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B1%82%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB%EF%BC%88Hierarchical-Agglomerative-Clustering-HAC%EF%BC%89%E5%8E%9F%E7%90%86"><span class="toc-text">分层凝聚聚类（Hierarchical Agglomerative Clustering,HAC）原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-Principle-Component-Analysis-PCA"><span class="toc-text">主成分分析(Principle Component Analysis,PCA)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%A4%A7%E6%96%B9%E5%B7%AE%E5%8E%9F%E7%90%86"><span class="toc-text">基于最大方差原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%B0%8F%E5%8C%96%E8%AF%AF%E5%B7%AE%E5%8E%9F%E7%90%86"><span class="toc-text">基于最小化误差原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PCA%E4%B8%8ELDA-Linear-Discriminant-Analysis-%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">PCA与LDA(Linear Discriminant Analysis)的比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PCA%E5%92%8C%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3-Non-negative-Matrix-Factorization-NMF-%E6%AF%94%E8%BE%83"><span class="toc-text">PCA和非负矩阵分解(Non-negative Matrix Factorization,NMF)比较</span></a></li></ol>
  </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header"><i class="iconfont icon-wenzhang_huaban" style="padding-right: 2px;"></i>最近文章</div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-01-04</div>
        <a href="/Algorithm-brush-notes/"><div class="recent-posts-item-content">无意中发现一位大佬的算法刷题pdf笔记</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-12-31</div>
        <a href="/100万的小目标/"><div class="recent-posts-item-content">第一个100万？</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-12-30</div>
        <a href="/javaweb-source-code/"><div class="recent-posts-item-content">分享两个可供练手的Javaweb网站源码</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-12-30</div>
        <a href="/Structure-learning/"><div class="recent-posts-item-content">机器学习之结构学习详解</div></a>
      </div>
    
  </div>
</div>
  </article>

  
  
  
</div>
          </aside>
        </div>
      </div>
    </div>
  </div>
  
  <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>Copyright ©
          
          2020 -
          
          2021
        </span>
        &nbsp;
        <a href="/" class="footer-link">IT&nbsp; 码&nbsp; 农 </a>
      </div>
		<div class="footer-dsc">
			<center><span>
			<a href="https://tanqingbo.cn/CSBook001/" target="_blank">计算机专业最全电子书PDF下载</a> - <a href="https://tanqingbo.cn/links/" target="_blank">友情链接</a>
			</span></center>
		</div>
    </div>

    
    
    
	
    
	

	<div class="footer-dsc">
	<script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>
	<span id="busuanzi_container_site_pv">
		本站总访问量<span id="busuanzi_value_site_pv"></span>次
	</span> &nbsp;|&nbsp
	<span id="busuanzi_container_site_uv">
		本站总访客数<span id="busuanzi_value_site_uv"></span>次
	</span>
	</div>
	
	
	
</footer>
  <a role="button" id="scrollbutton" class="basebutton" >
  <i class="iconfont icon-arrowleft button-icon"></i>
</a>
<a role="button" id="menubutton" class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a role="button" id="popbutton" class="basebutton">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a role="button" id="darkbutton" class="basebutton darkwidget">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a role="button" id="searchbutton" class="basebutton searchwidget">
  <i class="iconfont icon-search button-icon"></i>
</a>

  
  
  

  
  
  <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img');
    var i;
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a');
      wrapper.setAttribute('data-fslightbox', 'gallery');
      wrapper.setAttribute('href', img[i].getAttribute('data-src'));
      wrapper.style.cssText = 'width: 100%; display: flex; justify-content: center;';
      img[i].before(wrapper);
      wrapper.append(img[i]);
    }
    refreshFsLightbox();
  }
</script>
<script>loadScript("//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js", addImgLayout)</script>
  
  
  
<script src="/js/main.js"></script>

  
  
  <script>
    var addLazyload = function () {
      var observer = lozad('.lozad', {
        load: function (el) {
          el.srcset = el.getAttribute('data-src');
        },
        loaded: function (el) {
          el.classList.add('loaded');
        }
      });
      observer.observe();
    }
  </script>
  <script>loadScript("/js/lib/lozad.min.js", addLazyload)</script>
  
  
  <script>
    var googleAnalytics = function() {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-178024758-1');
    }
  </script>
  <script>loadScript("https://www.googletagmanager.com/gtag/js?id=" + "UA-178024758-1", googleAnalytics)</script>
  
</body>

</html>